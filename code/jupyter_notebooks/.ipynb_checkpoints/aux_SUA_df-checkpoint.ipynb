{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1ed4c08-2089-4e41-b8bf-01fda9e50b01",
   "metadata": {},
   "source": [
    "# Auxiliary notebook for a SUA properties calculation script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bea475c-3c6c-4c72-a4ee-df426988b615",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b363c3d9-a470-4868-b4ac-73003d9ac547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir('/CSNG/studekat/ripple_band_project/code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4524dc0d-cb81-471d-8400-fb13ebea12bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_analysis import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import pickle\n",
    "import neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d81388d2-3c1e-4411-b216-55216dad7272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f144a260-7100-48fd-b642-8271a5fd58f3",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f05d0790-3dc0-460f-bb79-ebd5a2d03adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/CSNG/studekat/ripple_band_project/code/params_analysis.yml\") as f:\n",
    "    params = yaml.safe_load(f)\n",
    "### AUX = params['aux']\n",
    "\n",
    "DATA_FOLDER = params['data_folder'] ### folder with all the preprocessed data\n",
    "DATES = params['dates']\n",
    "PEAK_BORDERS = params['phase_peak_borders']\n",
    "WIDTH_INTERVALS = params['width_intervals'] #[(0,7),(8,12),(13,90)]\n",
    "FINAL_CLASSES = ['DOWN_narrow_peak','DOWN_narrow_other','DOWN_medium','DOWN_wide','UP_peak','UP_other']\n",
    "\n",
    "DF_FOLDER = '/CSNG/studekat/ripple_band_project/dataframes' ### here the resulting dataframes will be saved\n",
    "MONKEY_LIST = ['L','N','F','A']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2547a4cf-3862-45ce-91b4-5f18bbfa7029",
   "metadata": {},
   "source": [
    "## Data handling sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf89703d-fd01-4789-91d3-8c075dbe1092",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = load_block('N',1,type_rec='RS',type_sig='spikes',date=DATES['N']['RS'][0],data_folder=DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc4568bc-708a-4d70-8e60-b8d416c191f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(block.segments[0].spiketrains)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cc0c15-acae-4915-a233-c732208fcf46",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a71503b2-b509-47e0-91f8-c28782c10afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spike_train_prop_vec(spike_vector,rb_phase,rb_envelope,rb_env_phase,channel_prop=None):\n",
    "    \"\"\"\n",
    "    Calculates properties of one spiketrain.\n",
    "\n",
    "    spike_train is a spike train directly from the nix file, with all metadata included\n",
    "    \"\"\"\n",
    "    from copy import deepcopy \n",
    "    \n",
    "    ### lists of RB phases and envs. of spikes - CAREFUL WITH MULTIPLE SPIKES IN ONE MS\n",
    "    phases_list = []\n",
    "    env_list = []\n",
    "    phases_env_list = []\n",
    "    aux_sp = deepcopy(spike_vector)\n",
    "    while np.sum(aux_sp)>0:\n",
    "        non_zero_idx = np.where(aux_sp>0)[0]\n",
    "        for idx in non_zero_idx:\n",
    "            phases_list.append(rb_phase[idx]) \n",
    "            env_list.append(rb_envelope[idx]) \n",
    "            phases_env_list.append(rb_env_phase[idx]) \n",
    "            aux_sp[idx]-=1 ### subtracting spikes that we have already used\n",
    "\n",
    "    ### phase preference\n",
    "    r, phi = circular_avg(np.array(phases_list),bins=30)\n",
    "    r_env, phi_env = circular_avg(np.array(phases_env_list),bins=30)\n",
    "\n",
    "    ### phases of high and low envelope spikes \n",
    "    high_env_mask = np.array(env_list)>=np.median(rb_envelope) #### mask from the envelope values for each spike (NOT IN SHAPE OF INPUT ARRAYS, only spikes)\n",
    "    low_env_mask = np.array(env_list)<np.median(rb_envelope)\n",
    "    \n",
    "    list_phases_high_env = np.array(phases_list)[high_env_mask]\n",
    "    list_phases_low_env = np.array(phases_list)[low_env_mask]\n",
    "\n",
    "    list_env_phases_high_env = np.array(phases_env_list)[high_env_mask]\n",
    "    list_env_phases_low_env = np.array(phases_env_list)[low_env_mask]\n",
    "\n",
    "    ### firing rate \n",
    "    dur_rec_ms = spike_vector.shape[0]\n",
    "    dur_rec_s = dur_rec_ms/1000\n",
    "    fr = np.sum(spike_vector)/dur_rec_s\n",
    "    fr_high_env = len(list_phases_high_env)/dur_rec_s*2 ### we normalise by 2, because this only considers spikes above median env.\n",
    "    fr_low_env = len(list_phases_low_env)/dur_rec_s*2\n",
    "    \n",
    "    ### CV ISI\n",
    "    len_intervals = count_zero_intervals(spike_vector) \n",
    "    CV_ISI = np.std(np.array(len_intervals))/np.mean(np.array(len_intervals))\n",
    "\n",
    "    ### average waveform\n",
    "    #avg_waveform = np.mean(spike_train.waveforms,axis=0)\n",
    "\n",
    "    prop_dict = {'FR':fr, \n",
    "                'CV_ISI': CV_ISI,\n",
    "                'ISI': len_intervals,\n",
    "\n",
    "                'env_th_median':np.median(rb_envelope), ### the median value of RB envelope on this channel\n",
    "                 \n",
    "                'list_phases': phases_list,\n",
    "                'list_env':env_list,\n",
    "                'list_env_phases':phases_env_list,\n",
    "\n",
    "                'list_phases_high_env':list_phases_high_env,\n",
    "                'list_env_phases_high_env':list_env_phases_high_env,\n",
    "                 \n",
    "                'list_phases_low_env':list_phases_low_env,\n",
    "                'list_env_phases_low_env':list_env_phases_low_env,\n",
    "\n",
    "                'FR_high_env_median':fr_high_env,\n",
    "                'FR_low_env_median':fr_low_env, \n",
    "                'FR_high_env_low_env_median_ratio':fr_high_env/fr_low_env, \n",
    "                 \n",
    "                'pref_phase_all_spikes':phi, \n",
    "                'norm_phase_sel_01_all_spikes':r, \n",
    "                'pref_env_phase_all_spikes': phi_env,\n",
    "                'norm_env_phase_sel_01_all_spikes': r_env,\n",
    "                 \n",
    "                #'avg_wf': avg_waveform,\n",
    "    }\n",
    "    ### adding other percentile TH values, so the spikes can be splitted into high/low env. in different ways later\n",
    "    for perc in [10,20,30,40,50,60,70,80,90,95]:\n",
    "        prop_dict[f'env_th_perc_{perc}'] = np.percentile(rb_envelope,perc)\n",
    "    \n",
    "    if channel_prop is not None:\n",
    "        for k in channel_prop.keys():\n",
    "            prop_dict[k] = channel_prop[k]\n",
    "    return prop_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10d5eaca-685a-4036-9685-235fdbc0627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aux_add_up_down_classes(df_sua):\n",
    "    \"\"\"\n",
    "    Clasifies whether the peak is UP or DOWN (bigger in abs. val above, or below 0), in the zscored waveform.\n",
    "    \"\"\"\n",
    "    df_added = df_sua\n",
    "    aux_classes = []\n",
    "    for idx in df_added.index:\n",
    "        wf = df_added.loc[idx]['avg_wf']\n",
    "        if np.abs(np.max(wf))>np.abs(np.min(wf)):\n",
    "            aux_classes.append('UP')\n",
    "        else:\n",
    "            aux_classes.append('DOWN')\n",
    "\n",
    "    df_added['wf_direction'] = aux_classes\n",
    "    return df_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e860b998-735c-4e4a-906f-c8f35cbc9a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aux_add_waveform_prop(df_sua):\n",
    "    \"\"\"\n",
    "    From the dataframe with formated waveform properties calculates waveform width and height (amplitude).\n",
    "    \"\"\"\n",
    "    df_added = df_sua\n",
    "    ### amplitude\n",
    "    waveforms = df_sua['avg_wf'].values\n",
    "    df_added['amp_wf'] = [np.max(wf) - np.min(wf) for wf in waveforms]\n",
    "    ### distance from peak to trough\n",
    "    min_idcs = [np.argmin(wf) for wf in waveforms]\n",
    "    df_added['width_wf'] = [np.abs(np.argmax(wf[min_idx:])+min_idx - np.argmin(wf)) for wf, min_idx in zip(waveforms,min_idcs)]\n",
    "    return df_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "301ae774-b7f1-4c22-aaef-81cd1a22f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aux_add_zscored_avg_waveform(df_sua):\n",
    "    \"\"\"\n",
    "    From a dataframe with formated waveforms, saves one more column with each waveform zscored, \n",
    "    and another column with its zscored amplitude.\n",
    "    \"\"\"\n",
    "    from scipy.stats import zscore\n",
    "    \n",
    "    waveforms = df_sua['avg_wf'].values\n",
    "    df_added = df_sua\n",
    "    df_added['avg_wf_zscored'] =  [zscore(wf.magnitude) for wf in waveforms]\n",
    "    wfs_zsc = df_added['avg_wf_zscored'].values\n",
    "    df_added['amp_wf_zscored'] = [np.max(wf) - np.min(wf) for wf in wfs_zsc]\n",
    "    \n",
    "    return df_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d0248de-06d3-43f4-8b4c-f5770e5e121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aux_add_width_classes(df_sua,width_intervals = WIDTH_INTERVALS):\n",
    "    \"\"\"\n",
    "    Adding width class info, based on the measured width of a waveform (peak to the right max.).\n",
    "    \"\"\"\n",
    "    names_widths = ['narrow','medium','wide']\n",
    "    df_added = df_sua\n",
    "    ### adding column with spike width classification into narrow, medium, wide\n",
    "    aux_classes = []\n",
    "    for idx in df_added.index:\n",
    "        width_row = df_added.loc[idx]['width_wf']\n",
    "        for i in range(len(width_intervals)):\n",
    "            interval = width_intervals[i]\n",
    "            if (width_row>=interval[0]) & (width_row<=interval[1]):\n",
    "                aux_classes.append(names_widths[i])\n",
    "    \n",
    "    df_added['width_wf_class'] = np.array(aux_classes)\n",
    "    return df_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58f984c7-178e-4d2e-8ced-55672df9857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### classification into 6 classes\n",
    "def aux_add_final_classes(df_sua,peak_borders=PEAK_BORDERS,final_classes=FINAL_CLASSES):\n",
    "    df_sua['final_class'] = 'NO_CLASS'\n",
    "    dict_cl_indices = {}\n",
    "    for cl in final_classes:\n",
    "        if cl=='DOWN_narrow_peak':\n",
    "            aux_df = sua_df_RS[sua_df_RS['wf_direction']=='DOWN']\n",
    "            aux_df = aux_df[aux_df['width_class']=='narrow']\n",
    "            mask_peak = (aux_df['pref_phase_all_spikes']>=peak_borders[0]) & (aux_df['pref_phase_all_spikes']<=peak_borders[1])\n",
    "            aux_df = aux_df[mask_peak]\n",
    "            dict_cl_indices['DOWN_narrow_peak'] = aux_df.index\n",
    "        elif cl=='DOWN_narrow_other':\n",
    "            aux_df = sua_df_RS[sua_df_RS['wf_direction']=='DOWN']\n",
    "            aux_df = aux_df[aux_df['width_class']=='narrow']\n",
    "            mask_peak = (aux_df['pref_phase_all_spikes']>=peak_borders[0]) & (aux_df['pref_phase_all_spikes']<=peak_borders[1])\n",
    "            aux_df = aux_df[~mask_peak]\n",
    "            dict_cl_indices['DOWN_narrow_other'] = aux_df.index\n",
    "        elif cl=='DOWN_medium':\n",
    "            aux_df = sua_df_RS[sua_df_RS['wf_direction']=='DOWN']\n",
    "            aux_df = aux_df[aux_df['width_class']=='medium']\n",
    "            dict_cl_indices['DOWN_medium'] = aux_df.index\n",
    "        elif cl=='DOWN_wide':\n",
    "            aux_df = sua_df_RS[sua_df_RS['wf_direction']=='DOWN']\n",
    "            aux_df = aux_df[aux_df['width_class']=='wide']\n",
    "            dict_cl_indices['DOWN_wide'] = aux_df.index\n",
    "        elif cl=='UP_peak':\n",
    "            aux_df = sua_df_RS[sua_df_RS['wf_direction']=='UP']\n",
    "            mask_peak = (aux_df['pref_phase_all_spikes']>=peak_borders[0]) & (aux_df['pref_phase_all_spikes']<=peak_borders[1])\n",
    "            aux_df = aux_df[mask_peak]\n",
    "            dict_cl_indices['UP_peak'] = aux_df.index\n",
    "        elif cl=='UP_other':\n",
    "            aux_df = sua_df_RS[sua_df_RS['wf_direction']=='UP']\n",
    "            mask_peak = (aux_df['pref_phase_all_spikes']>=peak_borders[0]) & (aux_df['pref_phase_all_spikes']<=peak_borders[1])\n",
    "            aux_df = aux_df[~mask_peak] \n",
    "            dict_cl_indices['UP_other'] = aux_df.index\n",
    "        else:\n",
    "            print('Undefined cell type.')\n",
    "    for cl in final_classes:\n",
    "        for i in dict_cl_indices[cl]:\n",
    "            df_sua.loc[i]['final_class'] = cl\n",
    "    return df_sua"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40464a2-4d6b-4581-bfbe-486280ec9881",
   "metadata": {},
   "source": [
    "## Dataframe calculation - the first part, computationaly expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e24b1c39-eaf7-41fe-b5b7-ae28e003bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for monkey in MONKEY_LIST:\n",
    "        print(monkey)\n",
    "        for date in params['dates'][monkey]['RS']:\n",
    "            print(date)\n",
    "            prop_list = []\n",
    "            for array in range(1,17): \n",
    "                print(array)\n",
    "                try:\n",
    "                    ### loading SUA spike trains and RB block\n",
    "                    try:\n",
    "                        spike_block = load_block(monkey,array,type_rec='RS',type_sig='spikes',date=date,data_folder=DATA_FOLDER) ### SUA\n",
    "                        RB_block = load_block(monkey,array,type_rec='RS',type_sig='RB',date=date,data_folder=DATA_FOLDER)\n",
    "                        num_cells = len(spike_block.segments[0].spiketrains)\n",
    "                        start_t_spikes_ms = int(np.floor(np.float64(spike_block.segments[0].spiketrains[0].t_start.magnitude)*1000))\n",
    "                        start_t_RB_ms = int(np.floor(np.float64(RB_block.segments[0].analogsignals[0].t_start.magnitude)*1000))\n",
    "                        print(f'Start t RB: {start_t_spikes_ms}')\n",
    "                        print(f'Start t spikes: {start_t_RB_ms}')\n",
    "                        if start_t_spikes_ms!=start_t_RB_ms:\n",
    "                            print('Spikes and ripples do not have the same start time.')\n",
    "                    except:\n",
    "                        print(f'Cannot read the spike file for date {date}, monkey {monkey}, array {array}.')\n",
    "                    try:\n",
    "                        df_OP = pd.read_csv(f'{DATA_FOLDER}/metadata/OP_maps_dataframes/{monkey}/OP_prop_OG_array{array}.csv')\n",
    "                    except:\n",
    "                        print(f'Cannot read OP maps for date {date}, monkey {monkey}, array {array}.')\n",
    "                    for cell in range(num_cells):\n",
    "                        spike_train = spike_block.segments[0].spiketrains[cell]\n",
    "                        cell_name = spike_train.annotations['nix_name']\n",
    "                        electrode_ID = spike_train.annotations['Electrode_ID']\n",
    "                        \n",
    "                        ### channel prop - additional info for a channel, such as OP, bad channel ID, array and area\n",
    "                        channel_prop = {}\n",
    "                        channel_prop['cell_name'] = cell_name\n",
    "                        ### OP\n",
    "                        try:\n",
    "                            ch_OP = df_OP[df_OP['Electrode_ID']==electrode_ID]\n",
    "                            if ch_OP['selectivity_01'].values[0]>0.2 and ch_OP['num_f0_high_jump'].values[0]<3:\n",
    "                                channel_prop['pref_OP'] = ch_OP['pref_OP'].values[0]\n",
    "                                channel_prop['selectivity_OP_01'] = ch_OP['selectivity_01'].values[0]\n",
    "                            else:\n",
    "                                channel_prop['pref_OP'] = np.nan\n",
    "                                channel_prop['selectivity_OP_01'] = ch_OP['selectivity_01'].values[0]\n",
    "                        except:\n",
    "                            channel_prop['pref_OP'] = np.nan\n",
    "                            channel_prop['selectivity_OP_01'] = np.nan\n",
    "                        ### channel order\n",
    "                        ch = aux_electrodeID_to_ch_order(monkey,date,electrode_ID,array,data_folder=DATA_FOLDER,type_rec='RS')\n",
    "                        channel_prop['channel_order'] = ch\n",
    "                        ### array\n",
    "                        channel_prop['array'] = array\n",
    "                        ### area\n",
    "                        if monkey in ['N','F']:\n",
    "                            name_area = 'Area'\n",
    "                        else:\n",
    "                            name_area = 'cortical_area'\n",
    "                        ch_area = spike_train.annotations[name_area]\n",
    "                        channel_prop['area'] = ch_area\n",
    "                        ### order in the spike train\n",
    "                        channel_prop['train_order'] = cell\n",
    "\n",
    "                        rb_phase_arr = sig_block_to_arr(RB_block,'RB_phase')\n",
    "                        rb_envelope_arr = sig_block_to_arr(RB_block,'RB_envelope_norm')\n",
    "                        rb_env_phase_arr = sig_block_to_arr(RB_block,'RB_envelope_phase')\n",
    "\n",
    "                        spike_arr = spike_block_to_arr(spike_block)\n",
    "\n",
    "                        ### cutting out common times only for N and F\n",
    "                        if monkey in ['N','F']:\n",
    "                            rb_phase_arr = cut_abs_times(rb_phase_arr,start_t_RB_ms,monkey,rec_type='RS',date=date,params=params)\n",
    "                            rb_envelope_arr = cut_abs_times(rb_envelope_arr,start_t_RB_ms,monkey,rec_type='RS',date=date,params=params)\n",
    "                            rb_env_phase_arr = cut_abs_times(rb_env_phase_arr,start_t_RB_ms,monkey,rec_type='RS',date=date,params=params)\n",
    "                            spike_arr = cut_abs_times(spike_arr,start_t_RB_ms,monkey,rec_type='RS',date=date,params=params)\n",
    "                        \n",
    "                        rb_phase = rb_phase_arr[ch,:]\n",
    "                        rb_envelope = rb_envelope_arr[ch,:]\n",
    "                        rb_env_phase = rb_env_phase_arr[ch,:]\n",
    "                        spike_vector = spike_arr[cell,:]\n",
    "                        \n",
    "                        prop_dict = spike_train_prop_vec(spike_vector,rb_phase,rb_envelope,rb_env_phase,channel_prop=channel_prop) ### input already binned spikes\n",
    "                        prop_list.append(prop_dict)\n",
    "                except:\n",
    "                    print(f'For array {array}, the SUA properties were not calculated.')\n",
    "            df_prop = pd.DataFrame(prop_list)\n",
    "            ensure_dir_exists(f'{DF_FOLDER}/sua_prop/')\n",
    "            df_prop.to_pickle(f'{DF_FOLDER}/sua_prop/monkey{monkey}_all_arrays_date_{date}.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3b07ec-b669-4d69-98d2-4273351af857",
   "metadata": {},
   "source": [
    "## Adding other properties and formating to the DF (computationaly easier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85bc19bf-8aa4-4f86-b038-79386c8e0439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L\n",
      "20170725\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'FINAL_CLASSES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m df_added \u001b[38;5;241m=\u001b[39m aux_add_width_classes(df_added,width_intervals\u001b[38;5;241m=\u001b[39mWIDTH_INTERVALS)\n\u001b[1;32m     12\u001b[0m df_added \u001b[38;5;241m=\u001b[39m aux_add_up_down_classes(df_added)\n\u001b[0;32m---> 13\u001b[0m df_added \u001b[38;5;241m=\u001b[39m aux_add_final_classes(df_added,peak_borders\u001b[38;5;241m=\u001b[39mPEAK_BORDERS,final_classes\u001b[38;5;241m=\u001b[39m\u001b[43mFINAL_CLASSES\u001b[49m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#### saving new dataframes with properties as pickle\u001b[39;00m\n\u001b[1;32m     16\u001b[0m ensure_dir_exists(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDF_FOLDER\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/sua_prop_all/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FINAL_CLASSES' is not defined"
     ]
    }
   ],
   "source": [
    "for monkey in ['L']: #MONKEY_LIST: \n",
    "    print(monkey)\n",
    "    all_RS_dates = params['dates'][monkey]['RS']\n",
    "    for date in [all_RS_dates[0]]:\n",
    "        print(date)\n",
    "        with open(f'{DF_FOLDER}/sua_prop/monkey{monkey}_all_arrays_date_{date}.pkl', \"rb\") as file:\n",
    "            df_sua = pickle.load(file)\n",
    "        df_added = aux_add_waveform_prop(df_sua)\n",
    "        df_added = aux_add_zscored_avg_waveform(df_added)\n",
    "        df_added = df_added[df_added['channel_order']>-1] ### erasing not working arrays\n",
    "        df_added = aux_add_width_classes(df_added,width_intervals=WIDTH_INTERVALS)\n",
    "        df_added = aux_add_up_down_classes(df_added)\n",
    "        df_added = aux_add_final_classes(df_added,peak_borders=PEAK_BORDERS,final_classes=FINAL_CLASSES)\n",
    "\n",
    "        #### saving new dataframes with properties as pickle\n",
    "        ensure_dir_exists(f'{DF_FOLDER}/sua_prop_all/')\n",
    "        df_added.to_pickle(f'{DF_FOLDER}/sua_prop_all/monkey{monkey}_all_arrays_date_{date}.pkl')\n",
    "        ### the copy warning is there only for the case of empty arrays, no worries about it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11415909-a1f1-4c68-a0d9-2d8607aa04c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FR', 'CV_ISI', 'ISI', 'env_th_median', 'list_phases', 'list_env',\n",
       "       'list_env_phases', 'list_phases_high_env', 'list_env_phases_high_env',\n",
       "       'list_phases_low_env', 'list_env_phases_low_env', 'FR_high_env_median',\n",
       "       'FR_low_env_median', 'FR_high_env_low_env_median_ratio',\n",
       "       'pref_phase_all_spikes', 'norm_phase_sel_01_all_spikes',\n",
       "       'pref_env_phase_all_spikes', 'norm_env_phase_sel_01_all_spikes',\n",
       "       'env_th_perc_10', 'env_th_perc_20', 'env_th_perc_30', 'env_th_perc_40',\n",
       "       'env_th_perc_50', 'env_th_perc_60', 'env_th_perc_70', 'env_th_perc_80',\n",
       "       'env_th_perc_90', 'env_th_perc_95', 'cell_name', 'pref_OP',\n",
       "       'selectivity_OP_01', 'channel_order', 'array', 'area', 'train_order'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sua.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6934d1a6-7ce1-4db9-ac3c-2549c8ba0842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05014339, -0.01445192,  0.02386775,  0.06520633,  0.10895354,\n",
       "        0.15288113,  0.19597583,  0.24087043,  0.29160523,  0.34895566,\n",
       "        0.40976766,  0.47132504,  0.5341738 ,  0.5987663 ,  0.6606145 ,\n",
       "        0.7127482 ,  0.75613725,  0.8050764 ,  0.87291396,  0.9466244 ,\n",
       "        0.9874277 ,  0.979754  ,  0.9792771 ,  1.0659401 ,  1.1943612 ,\n",
       "        1.1032836 ,  0.46750894, -0.7732454 , -2.2416277 , -3.3451507 ,\n",
       "       -3.7161076 , -3.4417903 , -2.890186  , -2.3654623 , -1.9425576 ,\n",
       "       -1.5552638 , -1.1426339 , -0.69371617, -0.22113544,  0.2478717 ,\n",
       "        0.66151655,  0.9617702 ,  1.1253848 ,  1.177942  ,  1.1656601 ,\n",
       "        1.1214588 ,  1.0578392 ,  0.98036045,  0.89750934,  0.8179574 ,\n",
       "        0.7446112 ,  0.6747624 ,  0.604675  ,  0.53231966,  0.457132  ,\n",
       "        0.37971666,  0.30235577,  0.2283751 ,  0.16010645,  0.09762747,\n",
       "        0.03965934, -0.01448944, -0.06378369, -0.10601135, -0.13959011,\n",
       "       -0.16521944, -0.18592702, -0.20517863, -0.22424632, -0.24103722,\n",
       "       -0.25207978, -0.25641543, -0.25702047, -0.25734344, -0.25714004,\n",
       "       -0.25332186, -0.24444708, -0.23241618, -0.21974275, -0.20714141,\n",
       "       -0.19425118, -0.18098997, -0.16713452, -0.15162452, -0.1335523 ,\n",
       "       -0.11382005, -0.09504234, -0.07929576, -0.06605677, -0.05283633],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zscore(df_sua['avg_wf'][0].magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c367a55-3ba0-434e-9e46-5d049da841b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4dea84f-5c9e-4d0f-a26b-e04f71fa6a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      13\n",
       "1       4\n",
       "2       5\n",
       "3      20\n",
       "4       5\n",
       "       ..\n",
       "499     8\n",
       "500     6\n",
       "501    12\n",
       "502     8\n",
       "503     8\n",
       "Name: width_wf, Length: 504, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sua['width_wf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "41e7ce46-9953-4581-8c13-c018fdd85d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 7], [8, 12], [13, 90]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WIDTH_INTERVALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04067af-3516-438c-aeb5-08bae4a1b3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
